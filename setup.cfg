[metadata]
name = evoeval
description = "EvoEval: Evolving Coding Benchmarks via LLM"
long_description = file: README.md
long_description_content_type = text/markdown
url = https://github.com/evo-eval/evoeval
license = Apache-2.0
license_file = LICENSE
platform = any
classifiers =
    Operating System :: OS Independent
    Programming Language :: Python :: 3
    License :: OSI Approved :: Apache Software License

[options]
packages = find:
python_requires = >=3.9
dependency_links =
install_requires =
    wget>=3.2
    tempdir>=0.7.1
    multipledispatch>=0.6.0
    appdirs>=1.4.4
    numpy>=1.19.5
    tqdm>=4.56.0
    termcolor>=2.0.0
    evalplus>=0.2.0

[options.entry_points]
console_scripts =
    evoeval.evaluate = evoeval.evaluate:main
